{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Face detection\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import argparse\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, add, Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.backend import common as K\n",
    "\n",
    "sys.setrecursionlimit(2 ** 20)\n",
    "np.random.seed(2 ** 10)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "class FaceCV(object):\n",
    "    \"\"\"\n",
    "    Singleton class for face recongnition task\n",
    "    \"\"\"\n",
    "    CASE_PATH = \".\\\\pretrained_models\\\\haarcascade_frontalface_alt.xml\"\n",
    "    WRN_WEIGHTS_PATH = \".\\\\pretrained_models\\\\weights.18-4.06.hdf5\"\n",
    "    \n",
    "\n",
    "\n",
    "    def __new__(cls, weight_file=None, depth=16, width=8, face_size=64):\n",
    "        if not hasattr(cls, 'instance'):\n",
    "            cls.instance = super(FaceCV, cls).__new__(cls)\n",
    "        return cls.instance\n",
    "\n",
    "    def __init__(self, depth=16, width=8, face_size=64):\n",
    "        self.face_size = face_size\n",
    "        self.model = WideResNet(face_size, depth=depth, k=width)()\n",
    "        model_dir = os.path.join(os.getcwd(), \"pretrained_models\").replace(\"//\", \"\\\\\")\n",
    "        fpath = get_file('weights.18-4.06.hdf5',\n",
    "                         self.WRN_WEIGHTS_PATH,\n",
    "                         cache_subdir=model_dir)\n",
    "        self.model.load_weights(fpath)\n",
    "\n",
    "    @classmethod\n",
    "    def draw_label(cls, image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   font_scale=1, thickness=2):\n",
    "        size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "        x, y = point\n",
    "        cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "        cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness)\n",
    "\n",
    "    def crop_face(self, imgarray, section, margin=40, size=64):\n",
    "        \"\"\"\n",
    "        :param imgarray: full image\n",
    "        :param section: face detected area (x, y, w, h)\n",
    "        :param margin: add some margin to the face detected area to include a full head\n",
    "        :param size: the result image resolution with be (size x size)\n",
    "        :return: resized image in numpy array with shape (size x size x 3)\n",
    "        \"\"\"\n",
    "        img_h, img_w, _ = imgarray.shape\n",
    "        if section is None:\n",
    "            section = [0, 0, img_w, img_h]\n",
    "        (x, y, w, h) = section\n",
    "        margin = int(min(w,h) * margin / 100)\n",
    "        x_a = x - margin\n",
    "        y_a = y - margin\n",
    "        x_b = x + w + margin\n",
    "        y_b = y + h + margin\n",
    "        if x_a < 0:\n",
    "            x_b = min(x_b - x_a, img_w-1)\n",
    "            x_a = 0\n",
    "        if y_a < 0:\n",
    "            y_b = min(y_b - y_a, img_h-1)\n",
    "            y_a = 0\n",
    "        if x_b > img_w:\n",
    "            x_a = max(x_a - (x_b - img_w), 0)\n",
    "            x_b = img_w\n",
    "        if y_b > img_h:\n",
    "            y_a = max(y_a - (y_b - img_h), 0)\n",
    "            y_b = img_h\n",
    "        cropped = imgarray[y_a: y_b, x_a: x_b]\n",
    "        resized_img = cv2.resize(cropped, (size, size), interpolation=cv2.INTER_AREA)\n",
    "        resized_img = np.array(resized_img)\n",
    "        return resized_img, (x_a, y_a, x_b - x_a, y_b - y_a)\n",
    "\n",
    "    def detect_face(self):\n",
    "        face_cascade = cv2.CascadeClassifier(self.CASE_PATH)\n",
    "\n",
    "        # 0 means the default video capture device in OS\n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        # infinite loop, break by key ESC\n",
    "        while True:\n",
    "            if not video_capture.isOpened():\n",
    "                sleep(50)\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = video_capture.read()\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.2,\n",
    "                minNeighbors=10,\n",
    "                minSize=(self.face_size, self.face_size)\n",
    "            )\n",
    "            # placeholder for cropped faces\n",
    "            face_imgs = np.empty((len(faces), self.face_size, self.face_size, 3))\n",
    "            for i, face in enumerate(faces):\n",
    "                face_img, cropped = self.crop_face(frame, face, margin=40, size=self.face_size)\n",
    "                (x, y, w, h) = cropped\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 200, 0), 2)\n",
    "                face_imgs[i,:,:,:] = face_img\n",
    "            if len(face_imgs) > 0:\n",
    "                # predict ages and genders of the detected faces\n",
    "                results = self.model.predict(face_imgs)\n",
    "                predicted_genders = results[0]\n",
    "                ages = np.arange(0, 101).reshape(101, 1)\n",
    "                predicted_ages = results[1].dot(ages).flatten()\n",
    "\n",
    "            # draw results\n",
    "            for i, face in enumerate(faces):\n",
    "                if (int(predicted_ages[i]))>=76:\n",
    "                    age=\"76-100\"\n",
    "                elif (int(predicted_ages[i]))>55:\n",
    "                    age=\"56-75\"\n",
    "                elif (int(predicted_ages[i]))>35:\n",
    "                    age=\"36-55\"\n",
    "                elif (int(predicted_ages[i]))>18:\n",
    "                    age=\"19-35\"\n",
    "                else:\n",
    "                    age=\"0-18\"\n",
    "                label = \"{}, {}\".format(age,\n",
    "                                        \"F\" if predicted_genders[i][0] > 0.3 else \"M\")\n",
    "                self.draw_label(frame, (face[0], face[1]), label)\n",
    "\n",
    "            cv2.imshow('PROJECT OUTPUT', frame)\n",
    "            if cv2.waitKey(5) == 27:  # ESC key press\n",
    "                break\n",
    "        # When everything is done, release the capture\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#def get_args():\n",
    "#    parser = argparse.ArgumentParser(description=\"This script detects faces from web cam input, \"\n",
    "#                                                \"and estimates age and gender for the detected faces.\",\n",
    "#                                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "#\n",
    "#   parser.add_argument(\"--depth\", type=int, default=16,\n",
    "#                        help=\"depth of network\")\n",
    "#    parser.add_argument(\"--width\", type=int, default=8,\n",
    "#                        help=\"width of network\")\n",
    "#    args = parser.parse_args()\n",
    "#    return args\n",
    "class WideResNet:\n",
    "    def __init__(self, image_size, depth=16, k=8):\n",
    "        self._depth = depth\n",
    "        self._k = k\n",
    "        self._dropout_probability = 0\n",
    "        self._weight_decay = 0.0005\n",
    "        self._use_bias = False\n",
    "        self._weight_init = \"he_normal\"\n",
    "\n",
    "        if K.image_data_format() == \"th\":\n",
    "            logging.debug(\"image_data_format = 'th'\")\n",
    "            self._channel_axis = 1\n",
    "            self._input_shape = (3, image_size, image_size)\n",
    "        else:\n",
    "            logging.debug(\"image_data_format = 'tf'\")\n",
    "            self._channel_axis = -1\n",
    "            self._input_shape = (image_size, image_size, 3)\n",
    "\n",
    "    # Wide residual network http://arxiv.org/abs/1605.07146\n",
    "    def _wide_basic(self, n_input_plane, n_output_plane, stride):\n",
    "        def f(net):\n",
    "            # format of conv_params:\n",
    "            #               [ [kernel_size=(\"kernel width\", \"kernel height\"),\n",
    "            #               strides=\"(stride_vertical,stride_horizontal)\",\n",
    "            #               padding=\"same\" or \"valid\"] ]\n",
    "            # B(3,3): orignal <<basic>> block\n",
    "            conv_params = [[3, 3, stride, \"same\"],\n",
    "                           [3, 3, (1, 1), \"same\"]]\n",
    "\n",
    "            n_bottleneck_plane = n_output_plane\n",
    "\n",
    "            # Residual block\n",
    "            for i, v in enumerate(conv_params):\n",
    "                if i == 0:\n",
    "                    if n_input_plane != n_output_plane:\n",
    "                        net = BatchNormalization(axis=self._channel_axis)(net)\n",
    "                        net = Activation(\"relu\")(net)\n",
    "                        convs = net\n",
    "                    else:\n",
    "                        convs = BatchNormalization(axis=self._channel_axis)(net)\n",
    "                        convs = Activation(\"relu\")(convs)\n",
    "\n",
    "                    convs = Conv2D(n_bottleneck_plane, kernel_size=(v[0], v[1]),\n",
    "                                          strides=v[2],\n",
    "                                          padding=v[3],\n",
    "                                          kernel_initializer=self._weight_init,\n",
    "                                          kernel_regularizer=l2(self._weight_decay),\n",
    "                                          use_bias=self._use_bias)(convs)\n",
    "                else:\n",
    "                    convs = BatchNormalization(axis=self._channel_axis)(convs)\n",
    "                    convs = Activation(\"relu\")(convs)\n",
    "                    if self._dropout_probability > 0:\n",
    "                        convs = Dropout(self._dropout_probability)(convs)\n",
    "                    convs = Conv2D(n_bottleneck_plane, kernel_size=(v[0], v[1]),\n",
    "                                          strides=v[2],\n",
    "                                          padding=v[3],\n",
    "                                          kernel_initializer=self._weight_init,\n",
    "                                          kernel_regularizer=l2(self._weight_decay),\n",
    "                                          use_bias=self._use_bias)(convs)\n",
    "\n",
    "            # Shortcut Connection: identity function or 1x1 convolutional\n",
    "            #  (depends on difference between input & output shape - this\n",
    "            #   corresponds to whether we are using the first block in each\n",
    "            #   group; see _layer() ).\n",
    "            if n_input_plane != n_output_plane:\n",
    "                shortcut = Conv2D(n_output_plane, kernel_size=(1, 1),\n",
    "                                         strides=stride,\n",
    "                                         padding=\"same\",\n",
    "                                         kernel_initializer=self._weight_init,\n",
    "                                         kernel_regularizer=l2(self._weight_decay),\n",
    "                                         use_bias=self._use_bias)(net)\n",
    "            else:\n",
    "                shortcut = net\n",
    "\n",
    "            return add([convs, shortcut])\n",
    "\n",
    "        return f\n",
    "\n",
    "\n",
    "    # \"Stacking Residual Units on the same stage\"\n",
    "    def _layer(self, block, n_input_plane, n_output_plane, count, stride):\n",
    "        def f(net):\n",
    "            net = block(n_input_plane, n_output_plane, stride)(net)\n",
    "            for i in range(2, int(count + 1)):\n",
    "                net = block(n_output_plane, n_output_plane, stride=(1, 1))(net)\n",
    "            return net\n",
    "\n",
    "        return f\n",
    "\n",
    "#    def create_model(self):\n",
    "    def __call__(self):\n",
    "        logging.debug(\"Creating model...\")\n",
    "\n",
    "        assert ((self._depth - 4) % 6 == 0)\n",
    "        n = (self._depth - 4) / 6\n",
    "\n",
    "        inputs = Input(shape=self._input_shape)\n",
    "\n",
    "        n_stages = [16, 16 * self._k, 32 * self._k, 64 * self._k]\n",
    "\n",
    "        conv1 = Conv2D(filters=n_stages[0], kernel_size=(3, 3),\n",
    "                              strides=(1, 1),\n",
    "                              padding=\"same\",\n",
    "                              kernel_initializer=self._weight_init,\n",
    "                              kernel_regularizer=l2(self._weight_decay),\n",
    "                              use_bias=self._use_bias)(inputs)  # \"One conv at the beginning (spatial size: 32x32)\"\n",
    "\n",
    "        # Add wide residual blocks\n",
    "        block_fn = self._wide_basic\n",
    "        conv2 = self._layer(block_fn, n_input_plane=n_stages[0], n_output_plane=n_stages[1], count=n, stride=(1, 1))(conv1)\n",
    "        conv3 = self._layer(block_fn, n_input_plane=n_stages[1], n_output_plane=n_stages[2], count=n, stride=(2, 2))(conv2)\n",
    "        conv4 = self._layer(block_fn, n_input_plane=n_stages[2], n_output_plane=n_stages[3], count=n, stride=(2, 2))(conv3)\n",
    "        batch_norm = BatchNormalization(axis=self._channel_axis)(conv4)\n",
    "        relu = Activation(\"relu\")(batch_norm)\n",
    "\n",
    "        # Classifier block\n",
    "        pool = AveragePooling2D(pool_size=(8, 8), strides=(1, 1), padding=\"same\")(relu)\n",
    "        flatten = Flatten()(pool)\n",
    "        predictions_g = Dense(units=2, kernel_initializer=self._weight_init, use_bias=self._use_bias,\n",
    "                              kernel_regularizer=l2(self._weight_decay), activation=\"softmax\")(flatten)\n",
    "        predictions_a = Dense(units=101, kernel_initializer=self._weight_init, use_bias=self._use_bias,\n",
    "                              kernel_regularizer=l2(self._weight_decay), activation=\"softmax\")(flatten)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=[predictions_g, predictions_a])\n",
    "\n",
    "        return model\n",
    "def main():\n",
    "    #args = get_args()\n",
    "    #depth = args.depth\n",
    "    #width = args.width\n",
    "\n",
    "    face = FaceCV(depth=16, width=8)\n",
    "\n",
    "    face.detect_face()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
